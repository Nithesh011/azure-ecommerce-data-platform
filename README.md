# ğŸŸ¦ Azure E-Commerce Data Platform (Batch + Real-Time)

<p align="center">
  <img src="https://img.shields.io/badge/Azure-Cloud-blue?logo=microsoftazure&logoColor=white" alt="Azure"/>
  <img src="https://img.shields.io/badge/Synapse-Analytics-blue?logo=apache-spark&logoColor=white" alt="Synapse"/>
  <img src="https://img.shields.io/badge/Python-3.10-yellow?logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/ADF-Data%20Factory-blue?logo=microsoftazure&logoColor=white" alt="ADF"/>
  <img src="https://img.shields.io/badge/EventHub-Streaming-blue?logo=microsoftazure&logoColor=white" alt="Event Hub"/>
  <img src="https://img.shields.io/badge/LogicApps-Integration-blue?logo=microsoftazure&logoColor=white" alt="Logic Apps"/>
  <img src="https://img.shields.io/badge/ADLS-Gen2-blue?logo=microsoftazure&logoColor=white" alt="ADLS Gen2"/>
</p>

## ğŸ“Œ Project Overview

This project implements a real-world e-commerce data platform using Azure services.  
It includes both **batch ingestion** and **real time streaming** ingestion and follows the **Medallion Architecture** (RAW â†’ BRONZE â†’ SILVER â†’ GOLD).

The data source is **FakeStore API**, and the platform ingests:

- Products (batch)
- Users (batch)
- Carts/Orders (real-time streaming via Event Hub)

This project demonstrates end-to-end Data Engineering skills:  
âœ” Ingestion  
âœ” Streaming  
âœ” Data transformation  
âœ” Lakehouse architecture  
âœ” Notebook orchestration  
âœ” Security & role configuration  
âœ” Production-ready folder structure

## ğŸ— High-Level Architecture

<p align="center">
  <img src="architecture/high_level_architecture.png" width="250" alt="High-Level Architecture"/>
</p>

## ğŸ§± Medallion Architecture

<p align="center">
  <img src="architecture/medallion_architecture.png" width="250" alt="Medallion Architecture"/>
</p>

## âš¡ Streaming Architecture

<p align="center">
  <img src="architecture/streaming_flow.png" width="250" alt="Streaming Architecture"/>
</p>

## ğŸ”„ Notebook Orchestration Flow

<p align="center">
  <img src="architecture/orchestration_flow.png" width="250" alt="Notebook Orchestration Flow"/>
</p>

## ğŸ“ Repository Structure
azure-ecommerce-data-platform/
â”‚
â”œâ”€â”€ architecture/          # Architecture diagrams
â”‚   â”œâ”€â”€ high_level_architecture.png
â”‚   â”œâ”€â”€ medallion_architecture.png
â”‚   â”œâ”€â”€ streaming_flow.png
â”‚   â””â”€â”€ orchestration_flow.png
â”‚
â”œâ”€â”€ notebooks/             # Synapse Spark notebooks
â”‚   â”œâ”€â”€ 01_nb_bronze_batch_load.ipynb
â”‚   â”œâ”€â”€ 02_nb_bronze_stream_load.ipynb
â”‚   â”œâ”€â”€ 03_nb_silver_transform.ipynb
â”‚   â”œâ”€â”€ 04_nb_silver_order_details.ipynb
â”‚   â””â”€â”€ 05_nb_gold_metrics.ipynb
â”‚
â”œâ”€â”€ screenshots/           # Project screenshots
â”‚   â”œâ”€â”€ adf_pipeline.png
â”‚   â”œâ”€â”€ adf_synapse_orchestration.png
â”‚   â”œâ”€â”€ adls_foldertree.png
â”‚   â”œâ”€â”€ bronze_stream_folder.png
â”‚   â”œâ”€â”€ eventhub_dashboard.png
â”‚   â”œâ”€â”€ gold_layer_preview.png
â”‚   â”œâ”€â”€ logicapp_designer.png
â”‚   â”œâ”€â”€ logicapp_runhistory.png
â”‚   â”œâ”€â”€ silver_layer_preview.png
â”‚   â”œâ”€â”€ synapse_notebooks.png
â”‚   â””â”€â”€ synapse_stream_output.png
â”‚
â”œâ”€â”€ pipelines/             # ADF ARM templates
â”‚   â”œâ”€â”€ PL_FakeStore_Ingestion.json
â”‚   â””â”€â”€ PL_Synapse_Notebook_Runner.json
â”‚
â”œâ”€â”€ challenges.md          # Challenges & solutions document
â””â”€â”€ README.md              # Project documentation


## ğŸ“¥ Batch Ingestion (ADF)

âœ… **ADF Pipeline: PL_FakeStore_Ingestion**  
This pipeline fetches:

- `/products`
- `/users`

from FakeStore API and stores them in:

/raw/fakestore/products
/raw/fakestore/users


<p align="center">
  <img src="screenshots/adf_pipeline.png" width="600" alt="ADF Pipeline"/>
</p>

## ğŸ” Azure Role Assignments (Very Important)

Your project required multiple permissions:

1. **ADLS â†’ ADF Permissions**  
   To allow copying data:  
   - Storage Blob Data Contributor  
   - Storage Blob Data Reader

2. **Synapse â†’ ADF Triggering**  
   To trigger Notebooks:  
   - Synapse Administrator  
   - Synapse Apache Spark Administrator  

   ADFâ€™s service principal was granted these roles.

3. **Event Hub Permissions**  
   - Logic App â†’ Send  
   - Synapse â†’ Listen  
   - Testing â†’ Manage

4. **Synapse Workspace User**  
   You (the developer) had:  
   - Synapse Administrator  
   - Storage Blob Data Contributor

## ğŸ§© Challenges (summarized)

A full list is in [`challenges.md`](challenges.md), but key items include:  
- ğŸ”¸ Event Hub "Unauthorized" Error â†’ Solved by moving access policy from Namespace level to Entity level.  
- ğŸ”¸ Notebook Not Appearing in ADF â†’ Solved by assigning ADF service principal Synapse Administrator + Spark Administrator roles.  
- ğŸ”¸ Streaming JSON Flattening â†’ Fixed using `explode(col("products"))`  
- ğŸ”¸ Join type mismatch in Silver layer â†’ Resolved by casting `col("product_id").cast("int")`

## ğŸ§ª How to Run This Project

1. Deploy ADLS Gen2 â†’ Create container `datalake` with folders: `raw/`, `bronze/`, `silver/`, `gold/`  
2. Deploy ADF and configure linked services.  
3. Deploy Logic App (1-minute recurrence).  
4. Deploy Event Hub (1 consumer group).  
5. Deploy Synapse Workspace + Spark Pool.  
6. Upload all notebooks.  
7. Create Notebook Orchestration ADF Pipeline.  
8. Check Bronze â†’ Silver â†’ Gold outputs.

## ğŸ Conclusion

This project demonstrates:  
- Batch & streaming ingestion  
- ADLS Medallion architecture  
- Spark-based transformations  
- Event-driven design  
- Data engineering best practices  
- Production-grade Azure setup
